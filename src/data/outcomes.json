{
  "metadata": {
    "source": "EU AI Act Compliance Checker",
    "url": "https://ai-act-service-desk.ec.europa.eu/en/eu-ai-act-compliance-checker",
    "last_update_date": "2025-12-05",
    "scraped_date": "2026-02-06",
    "description": "Outcome/flag definitions. Flags are set during navigation and combined at END to produce the result display. structure_level indicates the section of the results page (role, risk_level, obligation). priority_weight determines display order.",
    "total_outcomes": 45
  },
  "outcomes": {
    "flag_outofscope": {
      "id": "flag_outofscope",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "Your AI system is likely to fall outside the scope of the AI Act, according to Article 2. However, please double check with the AI Act provisions or consult a lawyer to verify this.",
      "applicable_articles": [
        2
      ],
      "is_empty": false
    },
    "flag_notaimodel_result_output": {
      "id": "flag_notaimodel_result_output",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "not_applicable",
      "display_color": "gray",
      "text": "Your AI model likely does not qualify as a general-purpose AI model. The AI Act does not introduce any obligation for providers of AI models that do not qualify as general-purpose AI models.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_aimodel_obligations_result_output": {
      "id": "flag_aimodel_obligations_result_output",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "",
      "applicable_articles": [],
      "is_empty": true
    },
    "flag_risklevel_output_gpai_with_systemic_risk": {
      "id": "flag_risklevel_output_gpai_with_systemic_risk",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "general",
      "display_color": "blue",
      "text": "Your AI model likely qualifies as a general-purpose AI model with systemic risk.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_risklevel_output_gpai_without_systemic_risk": {
      "id": "flag_risklevel_output_gpai_without_systemic_risk",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "general",
      "display_color": "blue",
      "text": "Your AI model likely qualifies as a general-purpose AI model, but it’s unlikely to be classified as a general-purpose AI model with systemic risk.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_aimodel_obligations_systemicrisk_result_output": {
      "id": "flag_aimodel_obligations_systemicrisk_result_output",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "systemic_risk",
      "display_color": "red",
      "text": "Your AI model likely qualifies as a general-purpose AI model. In that case, you must comply with the obligations under Articles 53 and 54 AI Act, which means:\n\n- draw up and maintain technical documentation about the model, including details of the development process, to provide to the AI Office upon request. National competent authorities can also ask the AI Office to request information on their behalf when this information is needed for their supervisory tasks (see further Article 53(1)(a), Annex XI AI Act);\n- provide information and documentation to downstream AI system providers to help them understand the model's capabilities and limitations and comply with their own obligations (see further Article 53(1)(b), Annex XII AI Act);\n- put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act);\n- draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article 53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);\n- cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences and powers (see further Article 53(3) AI Act);\n- if established outside the EU, appoint an authorised representative in the Union before placing their model on the market (see further Article 54 AI Act).\n\nYour general-purpose AI model is also likely to present systemic risk. In that case, you must comply with the additional obligations under Article 55 AI Act, namely:\n\n- perform model evaluations using standardised protocols and state-of-the-art tools, including conducting and documenting adversarial testing of the model to identify and mitigate systemic risks (see further Article 55(1)(a) AI Act);\n- assess and mitigate possible systemic risks at Union level, including their sources, which may stem from the development, placing on market, or use of the model (see further Article 55(1)(b) AI Act);\n- track, document, and report, without undue delay, relevant information about serious incidents and possible corrective measures to the AI Office and, as appropriate, national competent authorities (see further Article 55(1)(c) AI Act);\n- ensure adequate cybersecurity protection for both the model and its physical infrastructure (see further Article 55(1)(d) AI Act).\n\nYou may sign and adhere to the [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai) to demonstrate compliance with the obligations listed above pursuant to Articles 53(1)(a), (b), and (c), and 55(1) AI Act.",
      "applicable_articles": [
        4,
        53,
        54,
        55
      ],
      "is_empty": false
    },
    "flag_aimodel_obligations_nosystemicrisk_result_output": {
      "id": "flag_aimodel_obligations_nosystemicrisk_result_output",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "Your AI model likely qualifies as a general-purpose AI model. In that case, you must comply with the obligations under Articles 53 and 54 AI Act, which means:\n\n- draw up and maintain technical documentation about the model, including details of the development process, to provide to the AI Office upon request. National competent authorities can also ask the AI Office to request information on their behalf when this information is needed for their supervisory tasks (see further Article 53(1)(a), Annex XI AI Act);\n- provide information and documentation to downstream AI system providers to help them understand the model's capabilities and limitations and comply with their own obligations (see further Article 53(1)(b), Annex XII AI Act);\n- put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act);\n- draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article 53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);\n- cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences and powers (see further Article 53(3) AI Act);\n- if established outside the EU, appoint an authorised representative in the Union before placing their model on the market (see further Article 54 AI Act).\n\nYou may sign and adhere to the Transparency and Copyright Chapters of the [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai) to demonstrate compliance with the obligations listed above pursuant to Article 53(1)(a), (b), and (c) AI Act.",
      "applicable_articles": [
        4,
        53,
        54
      ],
      "is_empty": false
    },
    "flag_obligations_provideranddeployer_ailiteracy": {
      "id": "flag_obligations_provideranddeployer_ailiteracy",
      "structure_level": "obligation",
      "priority_weight": 98,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "According to Article 4, you must ensure a sufficient level of AI literacy of your staff and other persons dealing with the operation and use of AI systems on your behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.",
      "applicable_articles": [
        4
      ],
      "is_empty": false
    },
    "flag_role_provider_gpai_output": {
      "id": "flag_role_provider_gpai_output",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered a provider of a general-purpose AI model under Article 3(3).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_role_authorized_representative_gpai_output": {
      "id": "flag_role_authorized_representative_gpai_output",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered an authorised representative under Article 3(5).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_obligations_gpai_outofscope_solescientific": {
      "id": "flag_obligations_gpai_outofscope_solescientific",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "",
      "applicable_articles": [],
      "is_empty": true
    },
    "flag_obligations_gpai_outofscope_researchtesting": {
      "id": "flag_obligations_gpai_outofscope_researchtesting",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "",
      "applicable_articles": [],
      "is_empty": true
    },
    "flag_obligations_opensource_provider": {
      "id": "flag_obligations_opensource_provider",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "open_source_exception",
      "display_color": "green",
      "text": "Your general-purpose AI model likely qualifies for the open-source exception. Therefore, you must only comply with the following obligations under Article 53 AI Act:\n\n- put in place a policy to comply with Union law on copyright and related rights, and in particular identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790 (see further Article 53(1)(c) AI Act). You may sign and adhere to the Copyright Chapter of the [General-Purpose AI Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai) to demonstrate compliance;\n- draw up and publish a sufficiently detailed summary of the content used for training the model (see further Article 53(1)(d) AI Act), according to the [template for general-purpose AI model providers to summarise their training content](https://digital-strategy.ec.europa.eu/en/news/commission-presents-template-general-purpose-ai-model-providers-summarise-data-used-train-their);\n- cooperate as necessary with the European Commission and the national competent authorities in the exercise of their competences and powers (see further Article 53(3) AI Act).",
      "applicable_articles": [
        4,
        53
      ],
      "is_empty": false
    },
    "flag_risklevel_aisystem_output": {
      "id": "flag_risklevel_aisystem_output",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "general",
      "display_color": "blue",
      "text": "Your AI system is likely covered by the AI Act as it does fall within the scope of Article 3 (1).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_risklevel_aisystem_nohighrisk_output": {
      "id": "flag_risklevel_aisystem_nohighrisk_output",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "limited_risk",
      "display_color": "#ffc107",
      "text": "Your AI system is likely not considered as high-risk. It is likely that your AI system poses no to minimal risks.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_ai_system_outsidescope": {
      "id": "flag_ai_system_outsidescope",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "Your AI system is likely not covered by the AI Act as it does not fall within the scope of Article 3 (1).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_provider": {
      "id": "flag_ai_system_role_provider",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as a provider under Article 3 (3).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_deployer": {
      "id": "flag_ai_system_role_deployer",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as a deployer under Article 3 (4).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_importer": {
      "id": "flag_ai_system_role_importer",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as an importer under Article 3 (6).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_distributor": {
      "id": "flag_ai_system_role_distributor",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as a distributor under Article 3 (7).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_authorisedrepre": {
      "id": "flag_ai_system_role_authorisedrepre",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as an authorised representative under Article 3 (5).",
      "applicable_articles": [
        3
      ],
      "is_empty": false
    },
    "flag_ai_system_role_productmanufacturer": {
      "id": "flag_ai_system_role_productmanufacturer",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "According to the AI Act, you are considered as a product manufacturer under Articles 2 (1) (e) and 3 (8).",
      "applicable_articles": [
        2
      ],
      "is_empty": false
    },
    "flag_aisystem_role_and_obligation_outofscope": {
      "id": "flag_aisystem_role_and_obligation_outofscope",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "The AI Act does not apply to natural persons using AI systems in the course of a purely personal non-professional activity (Article 2 (10)). Please note that any activity through which a natural person gains an economic benefit on a regular basis or is otherwise involved in a professional, business, trade, occupational or freelance activity should be considered as a ‘professional’ activity.",
      "applicable_articles": [
        2
      ],
      "is_empty": false
    },
    "flag_role_distriimportdeployer_becomeprovider_result": {
      "id": "flag_role_distriimportdeployer_becomeprovider_result",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "Under Article 25 (1) of the AI Act, even though you initially identified yourself as a distributor, importer or deployer, you are likely considered a provider and must therefore meet the obligations associated with a provider.",
      "applicable_articles": [
        25
      ],
      "is_empty": false
    },
    "flag_role_manutoprovider_output": {
      "id": "flag_role_manutoprovider_output",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "role_classification",
      "display_color": "blue",
      "text": "Under Article 25 (3) of the AI Act, even though you initially identified yourself as a product manufacturer, you are likely considered a provider and must therefore meet the obligations associated with a provider.",
      "applicable_articles": [
        25
      ],
      "is_empty": false
    },
    "flag_aisystem_obligations_exclusions_output": {
      "id": "flag_aisystem_obligations_exclusions_output",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "It is likely that your AI system falls outside the scope of the AI Act. You are not required to comply with the AI Act because your system is classified as one of the excluded systems mentioned in Article 2.",
      "applicable_articles": [
        2
      ],
      "is_empty": false
    },
    "flag_obligations_prohibitedsystems_result_output": {
      "id": "flag_obligations_prohibitedsystems_result_output",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "prohibited",
      "display_color": "darkred",
      "text": "Your AI system is likely prohibited under the AI Act, according to Article 5. As of February 2025, you are not allowed to place your AI system on the market, put it into service, or use it.",
      "applicable_articles": [
        5
      ],
      "is_empty": false
    },
    "flag_risklevel_aisystem_highrisk_output": {
      "id": "flag_risklevel_aisystem_highrisk_output",
      "structure_level": "risk_level",
      "priority_weight": 50,
      "risk_level": "high_risk",
      "display_color": "orange",
      "text": "Your AI system is likely a high-risk system under the AI Act following Article 6.",
      "applicable_articles": [
        6
      ],
      "is_empty": false
    },
    "flag_obligations_provider_results_high_risk": {
      "id": "flag_obligations_provider_results_high_risk",
      "structure_level": "obligation",
      "priority_weight": 69,
      "risk_level": "high_risk",
      "display_color": "orange",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. You are required to comply with the obligations outlined in Articles 8-15 of the AI Act, which pertain to high-risk AI systems. These requirements include:\n-\testablish a risk management system;\n-\tensure adequate data governance and management practices;\n-\tdraw up technical documentation;\n-\tensure traceability (record-keeping);\n-\tenable deployers to interpret your AI system's output;\n-\tenable human oversight;\n-\tensure an appropriate level of accuracy, robustness and cybersecurity.\nAdditionally, as indicated in Article 16, a provider must also:\n-\tindicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trademark, the address at which they can be contacted;\n-\thave a quality management system in place which complies with Article 17;\n-\tkeep the documentation referred to in Article 18; \n-\twhen under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;\n-\tensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43;\n-\tdraw up an EU declaration of conformity in accordance with Article 47;\n-\taffix the CE marking to the high-risk AI system, in accordance with Article 48;\n-\tcomply with the registration obligations referred to in Article 49(1);\n-\ttake the necessary corrective actions and provide information as required in Article 20;\n-\tcooperate with national competent authorities as required in Article 21.\n-\tensure that the high-risk AI system complies with accessibility requirements in accordance with Directives (EU) 2016/2102 and (EU) 2019/882.  \nIf you have selected several sectors within Annex III and one of the use cases is considered high-risk and the other(s) not, compliance with the whole Chapter 3 applies only to the high-risk use case you have selected and the requirements must be assessed in view of that intended purpose",
      "applicable_articles": [
        6,
        8,
        16,
        17,
        18,
        19,
        20,
        21,
        43,
        47,
        48,
        49
      ],
      "is_empty": false
    },
    "flag_obligations_authorisdrepre_results_high_risk": {
      "id": "flag_obligations_authorisdrepre_results_high_risk",
      "structure_level": "obligation",
      "priority_weight": 69,
      "risk_level": "high_risk",
      "display_color": "orange",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As an authorised representative of a provider of a high-risk AI system, you must, pursuant to Article 22:\n\n- verify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider;\n- keep at the disposal of the competent authorities, for a period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact details of the provider that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable, the certificate issued by the notified body;\n- provide a competent authority, upon a reasoned request, with all the information and documentation necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in the AI Act;\n- cooperate with competent authorities;\n- terminate the mandate if you consider or have reason to consider the provider to be acting contrary to its obligations pursuant to the AI Act.",
      "applicable_articles": [
        6,
        11,
        22,
        47
      ],
      "is_empty": false
    },
    "flag_obligations_importer_output": {
      "id": "flag_obligations_importer_output",
      "structure_level": "obligation",
      "priority_weight": 69,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As an importer of a high-risk AI system, before placing a high-risk AI system on the market, pursuant to Article 23 you must:\n-\tverify the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;\n-\tverify that the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n-\tverify that the system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;\n-\tverify the provider has appointed an authorised representative in accordance with Article 22(1);\nAdditionally, as indicated in Article 23, an importer must also;\n-\tnot place the AI system on the market if there is sufficient reason to consider that a high-risk AI system is not in conformity with the AI Act, or is falsified, or accompanied by falsified documentation;\n-\tindicate your name, registered trade name or registered trade mark, and the address at which you can be contacted on the high-risk AI system and on its packaging or its accompanying documentation;\n-\tensure that storage or transport conditions, where applicable, do not jeopardise compliance with the AI Act;\n-\tkeep for a period of 10 years after the high-risk AI system has been placed on the market or put into service, a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47;\n-\tcooperate with competent authorities.",
      "applicable_articles": [
        6,
        11,
        22,
        23,
        43,
        47
      ],
      "is_empty": false
    },
    "flag_obligations_distributor_output": {
      "id": "flag_obligations_distributor_output",
      "structure_level": "obligation",
      "priority_weight": 69,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As a distributor of a high-risk AI system, before placing a high-risk AI system on the market, pursuant to Article 24 you must:\n-\tverify that the high-risk AI system bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system have complied with their respective obligations;\n-\tdo not make the high-risk AI system available on the market if there is reason to consider that the AI system is not in conformity with the requirements set out in the AI Act;\n-\tensure that storage or transport conditions, where applicable, do not jeopardise compliance with the AI Act;\n-\ttake corrective actions necessary to bring into conformirty a high-risk system that is not compliant with the AI Act;\n-\tcooperate with competent authorities.",
      "applicable_articles": [
        6,
        24,
        47
      ],
      "is_empty": false
    },
    "flag_obligations_deployer_output": {
      "id": "flag_obligations_deployer_output",
      "structure_level": "obligation",
      "priority_weight": 69,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 6. As a deployer of a high-risk AI system, pursuant to Article 26 you must:\n-\ttake appropriate technical and organisational measures to ensure that AI systems are used in accordance with the instructions accompanying the AI systems; \n-\tassign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support;\n-\tto the extent the deployer exercises control over the input data, you shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system;\n-\tmonitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance with Article 72. Where there is a reason to consider that the use of the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk you must without undue delay inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system; \n-\tkeep the logs automatically generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection of personal data. Deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs as part of the documentation kept pursuant to the relevant Union financial service law.\nAdditionally:\n-\tfor high-risk AI system at the workplace, deployers who are employers shall inform workers' representatives and the affected workers that they will be subject to the use of the high-risk AI system;\n-\tin the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, the deployer of a high-risk AI system for post-remote biometric identification shall request an authorisation, ex-ante, or without undue delay and no later than 48 hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence. Each use shall be limited to what is strictly necessary for the investigation of a specific criminal offence;\n-\tdeployers of high-risk AI systems referred to in Annex III that make decisions or assist in making decisions related to natural persons shall inform the natural persons that they are subject to the use of the high-risk AI system. For high-risk AI systems used for law enforcement purposes Аrticle-13 of Directive (EU) 2016/680 shall apply;\n-\tcooperate with competent authorities.    \n\nIf you have selected several sectors within Annex III and one of the use cases is considered high-risk and the other(s) not, compliance with the whole Chapter 3 applies only to the high-risk use case you have selected and the requirements must be assessed in view of that intended purpose",
      "applicable_articles": [
        6,
        26,
        72
      ],
      "is_empty": false
    },
    "flag_ai_system_no_highrisk_frimpactassessement_provider_output": {
      "id": "flag_ai_system_no_highrisk_frimpactassessement_provider_output",
      "structure_level": "obligation",
      "priority_weight": 68,
      "risk_level": "high_risk",
      "display_color": "orange",
      "text": "According to Article 6 (4), as a provider, if you consider that your AI system, as referred to in Annex III, is not high-risk, you must document this assessment before the system is placed on the market or put into service. Upon request from national competent authorities, you must provide the documentation of the assessment. You are also subject to the registration obligation outlined in Article 49(2). \nAttention, an AI system referred to in Annex III shall always be considered to be high-risk where the AI system performs profiling of natural persons.",
      "applicable_articles": [
        6,
        49
      ],
      "is_empty": false
    },
    "flag_fr_impact_assessment_deployer": {
      "id": "flag_fr_impact_assessment_deployer",
      "structure_level": "obligation",
      "priority_weight": 67,
      "risk_level": "general",
      "display_color": "blue",
      "text": "Prior to deploying a high-risk AI system, deployers that are bodies governed by public law, or are private entities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shall perform an assessment of the impact on fundamental rights that the use of such system may produce. For that purpose, deployers shall perform an assessment based on Article 27 requirements.\nThe above obligation applies to the first use of the high-risk AI system. The deployer may, in similar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried out by provider. \nOnce the assessment has been performed, the deployer shall notify the market surveillance authority of its results.\nIf any of the obligations laid down in this Article is already met through the data protection impact assessment conducted pursuant to Article-35 of Regulation (EU) 2016/679 or Article-27 of Directive (EU) 2016/680, the fundamental rights impact assessment referred to in paragraph 1 of this Article shall complement that data protection impact assessment.",
      "applicable_articles": [
        27
      ],
      "is_empty": false
    },
    "flag_obligation_transparency_provider": {
      "id": "flag_obligation_transparency_provider",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "transparency_obligations",
      "display_color": "blue",
      "text": "Following Article 50 of the AI Act, you must follow certain transparency requirements as a provider.\n- If your AI system interacts directly with natural persons (e.g. a chatbot), you must ensure that AI systems are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system. \nUnless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. \n\nThis obligation does not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.\n\n- If your AI system generates synthetic audio, image, video or text content you must ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated. You must ensure that your technical solutions are effective, interoperable, robust and reliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the costs of implementation and the generally acknowledged state of the art, as may be reflected in relevant technical standards. \n\nTransparency obligations shall not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law to detect, prevent, investigate or prosecute criminal offences. \nThe information shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure. The information shall conform to the applicable accessibility requirements.",
      "applicable_articles": [
        50
      ],
      "is_empty": false
    },
    "flag_obligation_transparency_deployer": {
      "id": "flag_obligation_transparency_deployer",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "transparency_obligations",
      "display_color": "blue",
      "text": "Following Article 50 of the AI Act, you must follow certain transparency requirements as a deployer.\n-\tIf you are deploying an emotion recognition system or a biometric categorisation system, you must inform the natural persons exposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation does not apply to AI systems used for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or investigate criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and in accordance with Union law. \n\n-\tIf your AI system generates or manipulates image, audio or video content constituting a deep fake, you must disclose that the content has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence. Where the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.\n\n-\tIf your AI system generates or manipulates text which is published with the purpose of informing the public on matters of public interest shall disclose that the text has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences or where the AI-generated content has undergone a process of human review or editorial control and where a natural or legal person holds editorial responsibility for the publication of the content. \n\nThe information should be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure. The information shall conform to the applicable accessibility requirements.",
      "applicable_articles": [
        50
      ],
      "is_empty": false
    },
    "flag_obligations_aisystem_opensource": {
      "id": "flag_obligations_aisystem_opensource",
      "structure_level": "obligation",
      "priority_weight": 70,
      "risk_level": "open_source_exception",
      "display_color": "green",
      "text": "The AI Act does not apply to AI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 (prohibited AI systems) or Article 50 (transparency obligations). See Article 2 (12).",
      "applicable_articles": [
        2,
        5,
        50
      ],
      "is_empty": false
    },
    "flag_obligations_aisystem_nohighrisk_output": {
      "id": "flag_obligations_aisystem_nohighrisk_output",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "limited_risk",
      "display_color": "#ffc107",
      "text": "Your AI system is likely not considered as high-risk. It is likely that your AI system does not have to comply with Chapter 3 of the AI Act, which relates to high-risk AI systems.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_obligations_authorisedrep": {
      "id": "flag_obligations_authorisedrep",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "Following Article 54 of the AI Act, the authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the AI Office upon request. For the purposes of the AI Act, the mandate shall empower the authorised representative to carry out the following tasks:\n\n- verify that the technical documentation specified in Annex XI has been drawn up and all obligations referred to in Article 53 and, where applicable, Article 55 have been fulfilled by the provider;\n\n- keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Office and national competent authorities, for a period of 10 years after the general-purpose AI model has been placed on the market, and the contact details of the provider that appointed the authorised representative;\n\n- provide the AI Office, upon a reasoned request, with all the information and documentation, including that referred to in the previous point, necessary to demonstrate compliance with the obligations in the AI Act;\n\n- cooperate with the AI Office and competent authorities.\n\nThe authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to the AI Act. In such a case, it shall also immediately inform the AI Office about the termination of the mandate and the reasons therefor.",
      "applicable_articles": [
        53,
        54,
        55
      ],
      "is_empty": false
    },
    "flag_ai_system_obligations_productmanufacturer": {
      "id": "flag_ai_system_obligations_productmanufacturer",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "obligations",
      "display_color": "blue",
      "text": "If you are a product manufacturer and do not fall under the conditions of Article 25(3) of the AI Act, you likely do not have obligations under the AI Act.",
      "applicable_articles": [
        25
      ],
      "is_empty": false
    },
    "flag_outofscope_gpai": {
      "id": "flag_outofscope_gpai",
      "structure_level": "role",
      "priority_weight": 50,
      "risk_level": "out_of_scope",
      "display_color": "gray",
      "text": "You are likely not a provider or an authorised representative of a provider under the AI Act, so you are not subject to the obligations for these roles.",
      "applicable_articles": [],
      "is_empty": false
    },
    "flag_obligations_provider_section_b_results_high_risk": {
      "id": "flag_obligations_provider_section_b_results_high_risk",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "high_risk",
      "display_color": "orange",
      "text": "Your AI system is likely classified as a high-risk system under the AI Act, following Article 2 (2) and Article 6 (1). You are required to comply with the requirements outlined in Article 8 to Article 15 of the AI Act; however, these articles need to be transposed into the relevant sectoral legislation, in accordance with Article 102 to Article 109, depending on the sectoral legislation applicable to the case at hand.",
      "applicable_articles": [
        2,
        6,
        8,
        15,
        102,
        109
      ],
      "is_empty": false
    },
    "flag_obligations_authorisedrep_nosystemicrisk_opensource": {
      "id": "flag_obligations_authorisedrep_nosystemicrisk_opensource",
      "structure_level": "obligation",
      "priority_weight": 50,
      "risk_level": "open_source_exception",
      "display_color": "green",
      "text": "Your general-purpose AI model likely qualifies for the open-source exception. Pursuant to Article 54 (6) AI Act there are no obligations for authorised representatives of an open-source, general-purpose AI model.",
      "applicable_articles": [
        54
      ],
      "is_empty": false
    },
    "flag_downstream_modifier": {
      "id": "flag_downstream_modifier",
      "structure_level": "obligation",
      "priority_weight": 49,
      "risk_level": "general",
      "display_color": "blue",
      "text": "The obligations under Article 53(1) AI Act are limited to the modifications, see Section 3.2.1 of the European Commission's [guidelines on the scope of the obligations for general-purpose AI models](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-07/guidelines_on_the_scope_of_the_obligations_for_generalpurpose_ai_models_established_by_regulation_1cx2atxgq79us4n3x8jfgyy1qlm_118340-3.pdf#page=21).",
      "applicable_articles": [
        53
      ],
      "is_empty": false
    }
  }
}